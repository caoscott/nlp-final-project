{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Welcome To Colaboratory",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/caoscott/nlp-final-project/blob/master/colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5fCEDCU_qrC0"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"https://colab.research.google.com/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<h1>Welcome to Colaboratory!</h1>\n",
        "\n",
        "\n",
        "Colaboratory is a free Jupyter notebook environment that requires no setup and runs entirely in the cloud.\n",
        "\n",
        "With Colaboratory you can write and execute code, save and share your analyses, and access powerful computing resources, all for free from your browser."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "2fhs6GZ4qFMx"
      },
      "source": [
        "To execute the code in the above cell, select it with a click and then either press the ▷ button to the left of the code, or use the keyboard shortcut \"⌘/Ctrl+Enter\".\n",
        "\n",
        "All cells modify the same global state, so variables that you define by executing a cell can be used in other cells:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IrCb-SNknqk0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1570
        },
        "outputId": "ad461cf1-817c-4933-96d1-52d47e7f2a0f"
      },
      "source": [
        "import os\n",
        "\n",
        "if not 'train2014' in os.listdir('.'):\n",
        "  !sudo apt install -f aria2\n",
        "  \n",
        "  !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Train_mscoco.zip -q --show-progress\n",
        "  !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Questions_Val_mscoco.zip -q --show-progress\n",
        "  !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Train_mscoco.zip -q --show-progress\n",
        "  !wget https://s3.amazonaws.com/cvmlp/vqa/mscoco/vqa/v2_Annotations_Val_mscoco.zip -q --show-progress\n",
        "  !aria2c -x6 http://images.cocodataset.org/zips/train2014.zip \n",
        "  !aria2c -x6 http://images.cocodataset.org/zips/val2014.zip \n",
        "\n",
        "  !unzip -q '*.zip'"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following package was automatically installed and is no longer required:\n",
            "  libnvidia-common-410\n",
            "Use 'sudo apt autoremove' to remove it.\n",
            "The following additional packages will be installed:\n",
            "  libc-ares2\n",
            "The following NEW packages will be installed:\n",
            "  aria2 libc-ares2\n",
            "0 upgraded, 2 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 1,274 kB of archives.\n",
            "After this operation, 4,912 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libc-ares2 amd64 1.14.0-1 [37.1 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/universe amd64 aria2 amd64 1.33.1-1 [1,236 kB]\n",
            "Fetched 1,274 kB in 0s (3,106 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 76, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libc-ares2:amd64.\n",
            "(Reading database ... 130811 files and directories currently installed.)\n",
            "Preparing to unpack .../libc-ares2_1.14.0-1_amd64.deb ...\n",
            "Unpacking libc-ares2:amd64 (1.14.0-1) ...\n",
            "Selecting previously unselected package aria2.\n",
            "Preparing to unpack .../aria2_1.33.1-1_amd64.deb ...\n",
            "Unpacking aria2 (1.33.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Setting up libc-ares2:amd64 (1.14.0-1) ...\n",
            "Setting up aria2 (1.33.1-1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1) ...\n",
            "v2_Questions_Train_ 100%[===================>]   6.90M  27.9MB/s    in 0.2s    \n",
            "v2_Questions_Val_ms 100%[===================>]   3.33M  15.1MB/s    in 0.2s    \n",
            "v2_Annotations_Trai 100%[===================>]  20.70M  49.1MB/s    in 0.4s    \n",
            "v2_Annotations_Val_ 100%[===================>]  10.03M  34.6MB/s    in 0.3s    \n",
            "\n",
            "05/13 07:17:27 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            " *** Download Progress Summary as of Mon May 13 07:18:27 2019 *** \n",
            "=\n",
            "[#559999 8.5GiB/12GiB(67%) CN:5 DL:140MiB ETA:29s]\n",
            "FILE: /content/train2014.zip\n",
            "-\n",
            "\n",
            "\u001b[0m\n",
            "05/13 07:18:56 [\u001b[1;32mNOTICE\u001b[0m] Download complete: /content/train2014.zip\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "559999|\u001b[1;32mOK\u001b[0m  |   143MiB/s|/content/train2014.zip\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "05/13 07:18:58 [\u001b[1;32mNOTICE\u001b[0m] Downloading 1 item(s)\n",
            "\u001b[0m\n",
            "05/13 07:19:43 [\u001b[1;32mNOTICE\u001b[0m] Download complete: /content/val2014.zip\n",
            "\n",
            "Download Results:\n",
            "gid   |stat|avg speed  |path/URI\n",
            "======+====+===========+=======================================================\n",
            "40ceb3|\u001b[1;32mOK\u001b[0m  |   142MiB/s|/content/val2014.zip\n",
            "\n",
            "Status Legend:\n",
            "(OK):download completed.\n",
            "\n",
            "6 archives were successfully processed.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XfEMAjGvAvx0",
        "colab_type": "code",
        "outputId": "3c6c43e8-8fd6-41f2-f4ed-0eb8055cbadc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 427
        }
      },
      "source": [
        "!rm -rf nlp-final-project\n",
        "!git clone https://github.com/caoscott/nlp-final-project.git\n",
        "!cd nlp-final-project && git checkout 9fa14f5a9846eb5bbd56256f55ece101f7d9f54a\n",
        "!mv nlp-final-project/* ."
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'nlp-final-project'...\n",
            "remote: Enumerating objects: 21, done.\u001b[K\n",
            "remote: Counting objects:   4% (1/21)   \u001b[K\rremote: Counting objects:   9% (2/21)   \u001b[K\rremote: Counting objects:  14% (3/21)   \u001b[K\rremote: Counting objects:  19% (4/21)   \u001b[K\rremote: Counting objects:  23% (5/21)   \u001b[K\rremote: Counting objects:  28% (6/21)   \u001b[K\rremote: Counting objects:  33% (7/21)   \u001b[K\rremote: Counting objects:  38% (8/21)   \u001b[K\rremote: Counting objects:  42% (9/21)   \u001b[K\rremote: Counting objects:  47% (10/21)   \u001b[K\rremote: Counting objects:  52% (11/21)   \u001b[K\rremote: Counting objects:  57% (12/21)   \u001b[K\rremote: Counting objects:  61% (13/21)   \u001b[K\rremote: Counting objects:  66% (14/21)   \u001b[K\rremote: Counting objects:  71% (15/21)   \u001b[K\rremote: Counting objects:  76% (16/21)   \u001b[K\rremote: Counting objects:  80% (17/21)   \u001b[K\rremote: Counting objects:  85% (18/21)   \u001b[K\rremote: Counting objects:  90% (19/21)   \u001b[K\rremote: Counting objects:  95% (20/21)   \u001b[K\rremote: Counting objects: 100% (21/21)   \u001b[K\rremote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (15/15), done.\u001b[K\n",
            "remote: Total 194 (delta 12), reused 15 (delta 6), pack-reused 173\u001b[K\n",
            "Receiving objects: 100% (194/194), 16.59 MiB | 20.29 MiB/s, done.\n",
            "Resolving deltas: 100% (115/115), done.\n",
            "Note: checking out '9fa14f5a9846eb5bbd56256f55ece101f7d9f54a'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 9fa14f5 Created using Colaboratory\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lQMgy20r2Bhs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        },
        "outputId": "66bf6b0c-9b77-4387-a63e-d7499161b9b8"
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.9 GB  | Proc size: 119.8 MB\n",
            "GPU RAM Free: 15079MB | Used: 0MB | Util   0% | Total 15079MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_uMJgNqfLsw0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch \n",
        "import torch.nn as nn\n",
        "import torch.optim as optim \n",
        "from torch.utils import data\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "abWTsB7XLNME",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "outputId": "8400f34a-aca1-4685-a924-f73d398a6c15"
      },
      "source": [
        "import embedding\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "transform_train = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomResizedCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    normalize,\n",
        "])\n",
        "\n",
        "word_embedding_file = 'glove.6B.300d-relativized.txt'\n",
        "word_embeddings = embedding.read_word_embeddings(word_embedding_file)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Read in 17615 vectors of size 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pHVV9-wWLige",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import copy\n",
        "import cv2\n",
        "import io\n",
        "import os\n",
        "from PIL import Image\n",
        "import h5py\n",
        "import random\n",
        "\n",
        "class VQADataset(data.Dataset):\n",
        "\n",
        "    def __init__(self, dataset_path: str, transform, \n",
        "            word_embeddings: embedding.WordEmbeddings, mode: str = 'train'):\n",
        "        self.word_embeddings = word_embeddings\n",
        "        answer_frequency = defaultdict(int)\n",
        "        questions_dict = \\\n",
        "        json.loads(open('v2_OpenEnded_mscoco_{}2014_questions.json'.format(mode)).read())['questions']\n",
        "        annotations_dict = \\\n",
        "        json.loads(open('v2_mscoco_{}2014_annotations.json'.format(mode)).read())['annotations']\n",
        "        print(\"JSON loaded.\")\n",
        "\n",
        "        year = '2015' if mode == 'test' else '2014'\n",
        "        img_prefix = \"COCO_\" + mode + year + \"_\"\n",
        "        dataset_dict = {}\n",
        "        while questions_dict:\n",
        "            question = questions_dict.pop()\n",
        "            image_id = '{:012d}'.format(question['image_id'])\n",
        "            image_name = img_prefix + image_id + \".jpg\"\n",
        "            dataset_dict[question['question_id']] = {'question': question['question'], \n",
        "                                                     'image_name': image_name}\n",
        "        while annotations_dict:\n",
        "            annotation = annotations_dict.pop()\n",
        "            dataset_dict[annotation['question_id']]['multiple_choice_answer'] = copy.deepcopy(annotation['multiple_choice_answer'])\n",
        "            answer_frequency[annotation['multiple_choice_answer']] += 1\n",
        "        print(\"Combined questions and answers.\")\n",
        "        del questions_dict, annotations_dict\n",
        "\n",
        "        top_answers = sorted([(v, k) for k, v in answer_frequency.items()], reverse=True)[:1000]\n",
        "        print(\"Done sorting.\")\n",
        "        answer_to_idx = {ans: idx for idx, (_, ans) in enumerate(top_answers)}\n",
        "        del top_answers, answer_frequency\n",
        "        self.dataset = defaultdict(list)\n",
        "        \n",
        "        while dataset_dict:\n",
        "            k, data = dataset_dict.popitem()\n",
        "            if data['multiple_choice_answer'] in answer_to_idx:\n",
        "                data['answer_index'] = torch.tensor(answer_to_idx[data['multiple_choice_answer']])\n",
        "                self.dataset[data['image_name']].append(data)\n",
        "\n",
        "        del dataset_dict\n",
        "        print('Pruned examples that\\'s not part of top 1000 answer choices')\n",
        "        self.image_names = [k for k in self.dataset.keys()]\n",
        "        self.mode = mode\n",
        "        self.dataset_path = dataset_path\n",
        "        self.transform = transform\n",
        "        self.keys = None\n",
        "        self.shuffle()\n",
        "        self.last_image_name = ''\n",
        "        self.last_img = None\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.keys)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        image_name, data_idx = self.keys[idx]\n",
        "        data = self.dataset[image_name][data_idx]\n",
        "        question_embedding = torch.tensor([self.word_embeddings.get_embedding(word) for word in data['question']], dtype=torch.float)\n",
        "        if image_name == self.last_image_name:\n",
        "          img = self.last_img\n",
        "        else:\n",
        "          with open(os.path.join(self.dataset_path, image_name), 'rb') as f:\n",
        "            img = Image.open(f).convert('RGB')\n",
        "            self.last_img = img\n",
        "            self.last_image_name = image_name\n",
        "  #         f = os.path.join(self.dataset_path, image_name)\n",
        "  #         img = Image.fromarray(cv2.cvtColor(cv2.imread(f), cv2.COLOR_BGR2RGB))\n",
        "        question_embedding = F.pad(question_embedding, pad=(0, 0, 60-question_embedding.shape[0], 0))\n",
        "        t_img = self.transform(img)\n",
        "        return t_img, question_embedding, data['answer_index']\n",
        "    \n",
        "    def shuffle(self):\n",
        "        random.shuffle(self.image_names)\n",
        "        self.keys = [(image_name, idx)\n",
        "                     for image_name in self.image_names\n",
        "                     for idx in range(len(self.dataset[image_name]))]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JpRRbx08kkin",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 190
        },
        "outputId": "29f68de7-0769-443f-92a4-14d3376a3430"
      },
      "source": [
        "vqa_train = VQADataset('train2014', transform_train, word_embeddings, 'train')\n",
        "vqa_test = VQADataset('val2014', transform_test, word_embeddings, 'val')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "JSON loaded.\n",
            "Combined questions and answers.\n",
            "Done sorting.\n",
            "Pruned examples that's not part of top 1000 answer choices\n",
            "JSON loaded.\n",
            "Combined questions and answers.\n",
            "Done sorting.\n",
            "Pruned examples that's not part of top 1000 answer choices\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3a-MufroLUW1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_loader = DataLoader(\n",
        "    vqa_train,\n",
        "    batch_size=512, shuffle=False, \n",
        "    num_workers=3, drop_last=False\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(\n",
        "    vqa_test,\n",
        "    batch_size=512, shuffle=False, \n",
        "    num_workers=3, drop_last=False\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "in-V9SsGGgwO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import models\n",
        "model = models.FeedForward().cuda()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dV5akb1Gw__",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.CrossEntropyLoss().cuda()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=5e-4)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.1, patience=7, verbose=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "04t4wR7XohsI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(loader):\n",
        "  with torch.no_grad():\n",
        "    total = 0\n",
        "    correct = 0\n",
        "    for image, question, answer in loader:\n",
        "      image, question, answer = image.cuda(), question.cuda(), answer.cuda()\n",
        "      out = model(image, question)\n",
        "      _, predicted = torch.max(out.data, 1)\n",
        "      total += answer.size(0)\n",
        "      correct += predicted.eq(answer.data).sum().item()\n",
        "      del image, question, answer, out, predicted\n",
        "    return correct/total"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eaOX7AVvy0I1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "best_acc = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g8iKjILxHHqA",
        "colab_type": "code",
        "outputId": "5f8ccd98-dfab-4ebc-cb65-4abd6819cb32",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 654
        }
      },
      "source": [
        "import time\n",
        "from google.colab import files\n",
        "\n",
        "for epoch in range(0, 100):\n",
        "  train_correct = 0\n",
        "  train_total = 0\n",
        "  train_loss = 0\n",
        "  \n",
        "  epoch_start_time = time.time()\n",
        "  start_time = time.time()\n",
        "  for batch_idx, (image, question, answer) in enumerate(train_loader):\n",
        "    optimizer.zero_grad()\n",
        "    image, question, answer = image.cuda(), question.cuda(), answer.cuda()\n",
        "    \n",
        "    out = model(image, question)\n",
        "    loss = criterion(out, answer)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    \n",
        "    train_loss += loss.item()\n",
        "    _, predicted = torch.max(out.data, 1)\n",
        "    train_total += answer.size(0)\n",
        "    train_correct += predicted.eq(answer.data).sum().item()\n",
        "    \n",
        "    del image, question, answer, out, predicted\n",
        "    \n",
        "    print('\\r| Epoch [%3d] Iter[%3d] Time: [%.3f] Avg Time: [%.3f]'\n",
        "          '\\t\\tLoss: %.4f Acc@1: %.3f' \n",
        "              % (epoch, batch_idx+1, time.time()-start_time, \n",
        "                 (time.time()-epoch_start_time)/(batch_idx+1),\n",
        "                 loss.item(), 100.*train_correct/train_total), end='')\n",
        "    \n",
        "    start_time = time.time()\n",
        "    \n",
        "  epoch_start_time = time.time()\n",
        "  test_acc = accuracy(test_loader)\n",
        "  print('\\n| Epoch [%3d] Time: [%.3f] Avg Time: [%.3f] \\tTest Acc: %.3f' \n",
        "        % (epoch, time.time()-epoch_start_time, \n",
        "           (time.time()-epoch_start_time)/len(test_loader), test_acc))\n",
        "  \n",
        "  scheduler.step(test_acc)\n",
        "  \n",
        "  if best_acc < test_acc:\n",
        "    best_acc = test_acc\n",
        "    file = 'epoch[%d]acc[%d].pth' %(epoch, int(test_acc * 10000))\n",
        "    torch.save(model.state_dict(), file)\n",
        "    files.download(file)\n",
        "    \n",
        "  vqa_train.shuffle()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([2, 512, 512])\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-98a41201f310>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/models.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image, question)\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m#rnn_out = question_out[:, -1, :]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 32\u001b[0;31m         \u001b[0mrnn_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquestion_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mquestion_out\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m512\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     33\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_out\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0;31m#question_out.view(question_out.shape[0], question_out.shape[2])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (512) must match the size of tensor b (0) at non-singleton dimension 1"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OV_5fE5pbDQq",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    }
  ]
}